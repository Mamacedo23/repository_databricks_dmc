{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418b6a7f-7920-435f-b445-d12942d3dca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, coalesce, to_date, substring, initcap, trim, col, lit, lower, current_timestamp, to_timestamp\n",
    "from pyspark.sql.types import LongType, StringType, DateType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "76a6d97a-65e6-4185-81dd-c2ebd4321461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_fecha_registro_safe(fecha_col: str):\n",
    "    col_trimmed = trim(col(fecha_col))\n",
    "    \n",
    "    return coalesce(\n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"),\n",
    "               to_date(col_trimmed, 'yyyy-MM-dd')),\n",
    "        \n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\"),\n",
    "               to_date(col_trimmed, 'dd/MM/yyyy')),\n",
    "        \n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{2}-\\\\d{2}-\\\\d{4}$\"),\n",
    "               to_date(col_trimmed, 'MM-dd-yyyy')),\n",
    "        \n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{4}/\\\\d{2}/\\\\d{2}\"),\n",
    "               to_date(substring(col_trimmed, 1, 10), 'yyyy/MM/dd')),\n",
    "        \n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}$\"),\n",
    "               to_date(substring(col_trimmed, 1, 10), 'yyyy-MM-dd')),\n",
    "        \n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4} \\\\d{2}:\\\\d{2}:\\\\d{2}$\"),\n",
    "               to_date(substring(col_trimmed, 1, 10), 'dd/MM/yyyy')),\n",
    "        \n",
    "        \n",
    "        when(col_trimmed.rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\"),\n",
    "               to_date(col_trimmed, 'MM/dd/yyyy')),\n",
    "        \n",
    "        lit(None).cast(DateType())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de71c29-9a3d-475f-80e1-f26f6b880504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"webinar\"\n",
    "email_pattern = \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n",
    "fecha_actual = datetime.now().date()\n",
    "\n",
    "df = (\n",
    "    spark.table(f\"{catalog_name}.bronze.clientes\")\n",
    "    .withColumn(\"nombre\", initcap(trim(col(\"nombre\"))))\n",
    "    .withColumn(\"ciudad\",\n",
    "        when(col(\"ciudad\").isNull(), None)\n",
    "        .otherwise(initcap(trim(col(\"ciudad\"))))\n",
    "    )\n",
    "    .withColumn(\"email\",\n",
    "        when(col(\"email\").isNull(), None)\n",
    "        .when(lower(trim(col(\"email\"))) == \"null\", None)\n",
    "        .otherwise(lower(trim(col(\"email\"))))\n",
    "    )\n",
    "    .withColumn(\"email\",\n",
    "        when(col(\"email\").rlike(email_pattern), col(\"email\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    .withColumn(\"fecha_registro\", parse_fecha_registro_safe(\"fecha_registro\"))\n",
    "    .withColumn(\n",
    "            \"fecha_registro\",\n",
    "            when(\n",
    "                (col(\"fecha_registro\") > lit(fecha_actual)) | \n",
    "                (col(\"fecha_registro\") < lit(\"1900-01-01\")),\n",
    "                None\n",
    "            ).otherwise(col(\"fecha_registro\"))\n",
    "    )\n",
    "    .dropDuplicates([\"id_cliente\"])\n",
    "    .withColumn(\"updated_at\", current_timestamp())\n",
    "    .select(\n",
    "            col(\"id_cliente\").cast(LongType()),\n",
    "            col(\"nombre\").cast(StringType()),\n",
    "            col(\"email\").cast(StringType()),\n",
    "            col(\"ciudad\").cast(StringType()),\n",
    "            col(\"fecha_registro\").cast(DateType()),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.silver.clientes\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clientes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
